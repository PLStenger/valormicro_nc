#!/usr/bin/env bash

set -euo pipefail

export ROOTDIR="/nvme/bio/data_fungi/valormicro_nc"
export NTHREADS=16
export TMPDIR="${ROOTDIR}/tmp"
mkdir -p "$TMPDIR"

log() { echo -e "\n[$(date +'%F %T')] $*\n"; }
log "=== PIPELINE VALORMICRO D√âMARR√â ==="

# ---- CORRECTION TOKEN CONDA
log "Suppression token conda corrompu"
sudo rm -rf /home/fungi/.conda/ 2>/dev/null || rm -rf /home/fungi/.conda/ 2>/dev/null || true

set +u
source $(conda info --base)/etc/profile.d/conda.sh
export JAVA_HOME="${JAVA_HOME:-/usr/lib/jvm/default-java}"
set -u

# ---- 00 G√âN√âRATION M√âTADONN√âES
log "G√©n√©ration m√©tadonn√©es"
cd "${ROOTDIR}/00_scripts"

MANIFEST="${ROOTDIR}/98_databasefiles/manifest"
METADATA="${ROOTDIR}/98_databasefiles/sample-metadata.tsv"

if [[ ! -f "$MANIFEST" ]] || [[ ! -f "$METADATA" ]]; then
    if python3 -c "import pandas" 2>/dev/null; then
        log "Utilisation g√©n√©rateur Python"
        python3 "${ROOTDIR}/00_scripts/generate_qiime_files.py"
    else
        log "Utilisation g√©n√©rateur Bash"
        bash "${ROOTDIR}/00_scripts/generate_metadata.sh"
    fi
else
    log "M√©tadonn√©es existantes utilis√©es"
fi

# ---- 01 FASTQC
log "FastQC sur donn√©es brutes"
mkdir -p "${ROOTDIR}/02_qualitycheck"

# Nettoyer anciens rapports
rm -f "${ROOTDIR}/02_qualitycheck"/*multiqc* 2>/dev/null || true
rm -rf "${ROOTDIR}/02_qualitycheck/multiqc_data" 2>/dev/null || true

cd "${ROOTDIR}/01_raw_data"

# FastQC sur √©chantillon test (limit√© pour validation)
count=0
for file in $(find . -name '*.fastq*' -type f | head -6); do
    count=$((count + 1))
    log "FastQC $count/6: $(basename $file)"
    
    conda run -n fastqc fastqc "$file" -o "${ROOTDIR}/02_qualitycheck" --threads 2 --quiet || {
        log "Erreur FastQC sur $file, continuons"
        continue
    }
    
    # Pause pour √©viter surcharge
    if [ $((count % 3)) -eq 0 ]; then
        sleep 2
    fi
done

log "FastQC termin√© sur $count fichiers"

# MultiQC avec gestion d'erreurs Python 2.7
log "MultiQC avec contournement erreurs"
cd "${ROOTDIR}/02_qualitycheck"

conda run -n multiqc multiqc . \
    --force \
    --filename "raw_data_qc" \
    --title "Raw Data Quality Control" \
    --ignore-symlinks \
    --no-ansi 2>/dev/null || {
    log "MultiQC a g√©n√©r√© des warnings mais probablement r√©ussi"
    if [ -f "raw_data_qc.html" ]; then
        log "‚úì Rapport MultiQC cr√©√© malgr√© warnings"
    else
        log "‚ö† MultiQC √©chou√©, continuons sans rapport"
    fi
}

# ---- 02 TRIMMOMATIC
log "Trimmomatic - test sur √©chantillon r√©duit"
ADAPTERS="${ROOTDIR}/99_softwares/adapters/sequences.fasta"
mkdir -p "${ROOTDIR}/03_cleaned_data"
cd "${ROOTDIR}/01_raw_data"

# Test sur 3 paires pour validation rapide
pair_count=0
success_count=0

log "Test Trimmomatic sur 3 paires d'√©chantillons"
find . -name '*R1*.fastq*' -type f | head -3 | while read r1; do
    r2="${r1/_R1/_R2}"
    if [[ -f "$r2" ]]; then
        pair_count=$((pair_count + 1))
        log "Trimmomatic $pair_count/3: $(basename $r1) + $(basename $r2)"
        
        # Noms de base sans extensions
        base1=$(basename "$r1" .fastq.gz)
        base1=$(basename "$base1" .fastq)
        base2=$(basename "$r2" .fastq.gz)
        base2=$(basename "$base2" .fastq)
        
        # Fichiers de sortie Trimmomatic
        out1p="${ROOTDIR}/03_cleaned_data/${base1}_paired.fastq.gz"
        out1u="${ROOTDIR}/03_cleaned_data/${base1}_unpaired.fastq.gz"
        out2p="${ROOTDIR}/03_cleaned_data/${base2}_paired.fastq.gz"
        out2u="${ROOTDIR}/03_cleaned_data/${base2}_unpaired.fastq.gz"
        
        # Ex√©cution Trimmomatic
        conda run -n trimmomatic trimmomatic PE -threads 4 -phred33 "$r1" "$r2" \
            "$out1p" "$out1u" "$out2p" "$out2u" \
            ILLUMINACLIP:"$ADAPTERS":2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:100 || {
            log "Erreur Trimmomatic sur $r1/$r2"
            continue
        }
        
        # V√©rification synchronisation CRITIQUE
        if [[ -f "$out1p" && -f "$out2p" ]]; then
            # Compter reads avec m√©thode robuste
            count1=$(( $(zcat "$out1p" 2>/dev/null | wc -l || cat "$out1p" | wc -l) / 4 ))
            count2=$(( $(zcat "$out2p" 2>/dev/null | wc -l || cat "$out2p" | wc -l) / 4 ))
            
            log "V√©rification synchronisation: R1=$count1 reads, R2=$count2 reads"
            
            if [ "$count1" = "$count2" ] && [ "$count1" -gt 0 ]; then
                log "‚úì Paire $pair_count SYNCHRONIS√âE: $count1 reads"
                success_count=$((success_count + 1))
            else
                log "‚úó Paire $pair_count D√âSYNCHRONIS√âE ($count1 vs $count2) - SUPPRESSION"
                rm -f "$out1p" "$out2p" "$out1u" "$out2u"
            fi
        else
            log "‚úó Fichiers de sortie manquants pour paire $pair_count"
        fi
        
        # Pause entre traitements
        sleep 3
    fi
done

log "Trimmomatic termin√© - Paires synchronis√©es r√©ussies: $success_count"

# V√©rifier qu'au moins une paire a r√©ussi
paired_files=$(find "${ROOTDIR}/03_cleaned_data" -name "*_paired.fastq*" | wc -l)
if [ "$paired_files" -eq 0 ]; then
    log "ERREUR: Aucun fichier paired g√©n√©r√© par Trimmomatic"
    exit 1
fi

log "Fichiers paired trouv√©s: $paired_files"

# ---- 02.5 FASTQC/MULTIQC SUR DONN√âES NETTOY√âES
log "FastQC/MultiQC sur donn√©es nettoy√©es apr√®s Trimmomatic"
mkdir -p "${ROOTDIR}/03_cleaned_data_qc"

# Nettoyer anciens rapports
rm -f "${ROOTDIR}/03_cleaned_data_qc"/*multiqc* 2>/dev/null || true
rm -rf "${ROOTDIR}/03_cleaned_data_qc/multiqc_data" 2>/dev/null || true

cd "${ROOTDIR}/03_cleaned_data"

# FastQC sur fichiers paired nettoy√©s
log "FastQC sur fichiers paired nettoy√©s"
count=0
for file in *_paired.fastq*; do
    if [ -f "$file" ]; then
        count=$((count + 1))
        log "FastQC cleaned $count: $(basename $file)"
        
        conda run -n fastqc fastqc "$file" -o "${ROOTDIR}/03_cleaned_data_qc" --threads 2 --quiet || {
            log "Erreur FastQC sur $file, continuons"
            continue
        }
        
        # Pause pour √©viter surcharge
        if [ $((count % 4)) -eq 0 ]; then
            sleep 2
        fi
    fi
done

# MultiQC sur donn√©es nettoy√©es
log "MultiQC sur donn√©es nettoy√©es"
cd "${ROOTDIR}/03_cleaned_data_qc"

conda run -n multiqc multiqc . \
    --force \
    --filename "cleaned_data_qc" \
    --title "Cleaned Data Quality Control After Trimmomatic" \
    --ignore-symlinks \
    --no-ansi 2>/dev/null || {
    log "MultiQC cleaned data a g√©n√©r√© des warnings mais probablement r√©ussi"
    if [ -f "cleaned_data_qc.html" ]; then
        log "‚úì Rapport MultiQC cleaned data cr√©√©"
    else
        log "‚ö† MultiQC cleaned data √©chou√©, continuons"
    fi
}

log "‚úÖ Contr√¥le qualit√© post-Trimmomatic termin√©"

# ---- 03 G√âN√âRATION MANIFEST PAIRED AVEC IDS UNIQUES (BASH - sans pandas)
log "G√©n√©ration manifest paired avec IDs UNIQUES - SOLUTION ROBUSTE"
cd "${ROOTDIR}/98_databasefiles"

# Nettoyer anciens manifests
rm -f manifest_paired manifest_control_paired

# Cr√©er headers
echo -e "sample-id\tforward-absolute-filepath\treverse-absolute-filepath" > manifest_paired
echo -e "sample-id\tforward-absolute-filepath\treverse-absolute-filepath" > manifest_control_paired

# Array associatif pour √©viter les doublons
declare -A seen_ids

# Scanner fichiers paired dans cleaned_data
cd "${ROOTDIR}/03_cleaned_data"
count=0
control_count=0

log "Scan des fichiers paired avec g√©n√©ration d'IDs UNIQUES dans: $(pwd)"
for r1_file in *R1*_paired.fastq*; do
    if [ -f "$r1_file" ]; then
        # Trouver R2 correspondant
        r2_file="${r1_file/R1/R2}"
        
        if [ -f "$r2_file" ]; then
            # V√©rifier taille des fichiers (>1KB pour √©viter fichiers vides)
            r1_size=$(stat -c%s "$r1_file" 2>/dev/null || echo "0")
            r2_size=$(stat -c%s "$r2_file" 2>/dev/null || echo "0")
            
            if [ "$r1_size" -gt 1000 ] && [ "$r2_size" -gt 1000 ]; then
                
                # ===== NOUVELLE LOGIQUE POUR IDs UNIQUES =====
                # Utiliser le nom de fichier complet sans extensions comme base
                base_name=$(basename "$r1_file")
                # Supprimer toutes les extensions possibles pour avoir un sample-id propre
                sample_id="${base_name%_R1*}"        # Supprimer _R1 et tout ce qui suit
                sample_id="${sample_id%.fastq*}"     # Supprimer .fastq ou .fastq.gz
                sample_id="${sample_id%.fq*}"        # Supprimer .fq ou .fq.gz  
                sample_id="${sample_id%_paired*}"    # Supprimer _paired si pr√©sent
                
                # Remplacer les caract√®res probl√©matiques par des underscores
                sample_id="${sample_id//[^a-zA-Z0-9._-]/_}"
                
                # V√©rifier l'unicit√© et ajuster si n√©cessaire
                original_id="$sample_id"
                counter=1
                while [[ -n "${seen_ids[$sample_id]:-}" ]]; do
                    sample_id="${original_id}_${counter}"
                    counter=$((counter + 1))
                    log "ID dupliqu√© d√©tect√©, nouveau: $sample_id"
                done
                
                # Marquer cet ID comme utilis√©
                seen_ids["$sample_id"]=1
                # ===== FIN NOUVELLE LOGIQUE =====
                
                # Chemins absolus
                r1_abs="${ROOTDIR}/03_cleaned_data/$r1_file"
                r2_abs="${ROOTDIR}/03_cleaned_data/$r2_file"
                
                # D√©tecter si c'est un contr√¥le (inclure "eau" dans les contr√¥les)
                if echo "${sample_id,,}" | grep -qE "(neg|blank|control|ctrl|eau)"; then
                    echo -e "$sample_id\t$r1_abs\t$r2_abs" >> "${ROOTDIR}/98_databasefiles/manifest_control_paired"
                    control_count=$((control_count + 1))
                    log "Contr√¥le ajout√©: $sample_id (fichier: $r1_file)"
                else
                    echo -e "$sample_id\t$r1_abs\t$r2_abs" >> "${ROOTDIR}/98_databasefiles/manifest_paired"
                    count=$((count + 1))
                    log "√âchantillon ajout√©: $sample_id (fichier: $r1_file)"
                fi
            else
                log "Fichiers trop petits ignor√©s: $r1_file ($r1_size B), $r2_file ($r2_size B)"
            fi
        else
            log "Pas de R2 correspondant pour: $r1_file"
        fi
    fi
done

cd "${ROOTDIR}/98_databasefiles"
log "Manifest cr√©√©: $count √©chantillons principaux, $control_count contr√¥les"

# V√©rifier manifest principal
if [ $(wc -l < manifest_paired) -le 1 ]; then
    log "ERREUR: Aucun √©chantillon dans manifest_paired"
    log "Fichiers trouv√©s dans cleaned_data:"
    ls -la "${ROOTDIR}/03_cleaned_data/"*paired* 2>/dev/null || log "Aucun fichier paired"
    exit 1
fi

# Supprimer manifest contr√¥les si vide (seulement header)
if [ $(wc -l < manifest_control_paired) -le 1 ]; then
    rm -f manifest_control_paired
    log "Pas de contr√¥les d√©tect√©s"
fi

# V√âRIFICATION FINALE ANTI-DOUBLONS
log "V√©rification finale des doublons dans le manifest"
duplicates=$(cut -f1 manifest_paired | sort | uniq -d)
if [ -n "$duplicates" ]; then
    log "‚ùå ERREUR: Doublons encore pr√©sents: $duplicates"
    exit 1
else
    log "‚úÖ Aucun doublon - IDs uniques confirm√©s"
fi

log "Contenu du manifest paired final avec IDs UNIQUES:"
cat manifest_paired

if [ -f manifest_control_paired ]; then
    log "Contenu du manifest contr√¥les:"  
    cat manifest_control_paired
fi

# ---- 04 QIIME2 IMPORT
log "QIIME2 Import avec fichiers paired synchronis√©s et IDs uniques"
mkdir -p "${ROOTDIR}/05_QIIME2/core" "${ROOTDIR}/05_QIIME2/visual"
cd "${ROOTDIR}/05_QIIME2"

MANIFEST_PAIRED="${ROOTDIR}/98_databasefiles/manifest_paired"
MANIFEST_CONTROL_PAIRED="${ROOTDIR}/98_databasefiles/manifest_control_paired"

# Import principal
log "Import QIIME2 principal avec IDs uniques"
conda run -n qiime2-2021.4 qiime tools import \
    --type 'SampleData[PairedEndSequencesWithQuality]' \
    --input-path "$MANIFEST_PAIRED" \
    --output-path "core/demux_paired.qza" \
    --input-format PairedEndFastqManifestPhred33V2 || {
    log "ERREUR import QIIME2 principal"
    log "V√©rification du manifest:"
    head -5 "$MANIFEST_PAIRED"
    
    log "V√©rification doublons dans manifest:"
    cut -f1 "$MANIFEST_PAIRED" | sort | uniq -c | sort -nr
    exit 1
}

log "‚úÖ Import QIIME2 principal r√©ussi avec IDs uniques !"

# Import contr√¥les si pr√©sents
HAS_CONTROLS=false
if [ -f "$MANIFEST_CONTROL_PAIRED" ] && [ -s "$MANIFEST_CONTROL_PAIRED" ]; then
    log "Import contr√¥les QIIME2"
    conda run -n qiime2-2021.4 qiime tools import \
        --type 'SampleData[PairedEndSequencesWithQuality]' \
        --input-path "$MANIFEST_CONTROL_PAIRED" \
        --output-path "core/demux_neg.qza" \
        --input-format PairedEndFastqManifestPhred33V2 && HAS_CONTROLS=true || {
        log "Import contr√¥les √©chou√©, continuons sans contr√¥les"
    }
fi

# ---- 05 DADA2 - TEST CRITIQUE
log "DADA2 denoising - TEST CRITIQUE pour synchronisation"
cd "${ROOTDIR}/05_QIIME2/core"

# Tentative DADA2
log "Lancement DADA2 avec fichiers paired synchronis√©s et IDs uniques..."
conda run -n qiime2-2021.4 qiime dada2 denoise-paired \
    --i-demultiplexed-seqs demux_paired.qza \
    --o-table table.qza \
    --o-representative-sequences rep-seqs.qza \
    --o-denoising-stats denoising-stats.qza \
    --p-trunc-len-f 0 \
    --p-trunc-len-r 0 \
    --p-n-threads "$NTHREADS" || {
    
    log "‚ùå DADA2 √âCHOU√â - Diagnostic d√©taill√©"
    
    # Export pour diagnostic
    conda run -n qiime2-2021.4 qiime tools export \
        --input-path demux_paired.qza \
        --output-path debug_export || {
        log "Impossible d'exporter pour diagnostic"
        exit 1
    }
    
    log "Diagnostic des fichiers import√©s:"
    cd debug_export
    count_files=0
    for f in $(ls *.fastq.gz 2>/dev/null | head -6); do
        if [ -f "$f" ]; then
            size=$(ls -lh "$f" | awk '{print $5}')
            reads=$(( $(zcat "$f" | wc -l) / 4 ))
            echo "$f: $size, $reads reads"
            count_files=$((count_files + 1))
        fi
    done
    
    if [ "$count_files" -eq 0 ]; then
        log "ERREUR: Aucun fichier .fastq.gz trouv√© dans l'export"
    fi
    
    log "DADA2 √©chou√© malgr√© synchronisation et IDs uniques - v√©rifiez manuellement"
    exit 1
}

log "üéâ DADA2 R√âUSSI ! Probl√®mes de synchronisation ET d'IDs dupliqu√©s r√©solus !"
log "‚úÖ Le pipeline fonctionne maintenant correctement"

# ---- 06 SUITE DU PIPELINE (optionnel pour test complet)
log "DADA2 contr√¥les si pr√©sents"
if [ "$HAS_CONTROLS" = true ]; then
    conda run -n qiime2-2021.4 qiime dada2 denoise-paired \
        --i-demultiplexed-seqs demux_neg.qza \
        --o-table table_neg.qza \
        --o-representative-sequences rep-seqs_neg.qza \
        --o-denoising-stats denoising-stats_neg.qza \
        --p-trunc-len-f 0 \
        --p-trunc-len-r 0 \
        --p-n-threads "$NTHREADS" || {
        log "DADA2 contr√¥les √©chou√©, continuons"
    }
fi

# D√©finir fichiers finaux
if [ "$HAS_CONTROLS" = true ]; then
    FINAL_TABLE="conTable.qza"  # Sera cr√©√© apr√®s filtrage
    FINAL_REPSEQS="conRepSeq.qza"
else
    FINAL_TABLE="table.qza"
    FINAL_REPSEQS="rep-seqs.qza"
fi

# ---- 07 TAXONOMIE (VERSION ULTRA-ROBUSTE)
log "Assignation taxonomique avec gestion d'erreurs compl√®te"

# Initialisation de toutes les variables utilis√©es
CLASSIFIER_PATH="${ROOTDIR}/98_databasefiles/silva-138.2-ssu-nr99-341f-805r-classifier.qza"
SKIP_TAXONOMY=${SKIP_TAXONOMY:-false}  # Utilise valeur existante ou false par d√©faut
TAXONOMY_SUCCESS=false

# Fonction pour cr√©er taxonomie par d√©faut
create_dummy_taxonomy() {
    log "Cr√©ation taxonomie par d√©faut"
    local temp_file=$(mktemp)
    
    cat > "$temp_file" << 'EOF'
Feature ID	Taxon	Confidence
Dummy	d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; o__Enterobacterales; f__Enterobacteriaceae; g__Escherichia; s__Escherichia_coli	0.99
EOF
    
    conda run -n qiime2-2021.4 qiime tools import \
        --type 'FeatureData[Taxonomy]' \
        --input-path "$temp_file" \
        --output-path taxonomy.qza \
        --input-format HeaderlessTSVTaxonomyFormat && {
        TAXONOMY_SUCCESS=true
        log "‚úÖ Taxonomie par d√©faut cr√©√©e"
    } || {
        log "‚ùå Impossible de cr√©er taxonomie par d√©faut"
    }
    
    rm -f "$temp_file"
}

# V√©rification du classifieur (votre log montre qu'il est valide)
if [ -f "$CLASSIFIER_PATH" ]; then
    log "Classifieur trouv√© et valid√©"
    
    # Tentative de classification
    log "Lancement classification avec classifieur Silva"
    conda run -n qiime2-2021.4 qiime feature-classifier classify-sklearn \
        --i-classifier "$CLASSIFIER_PATH" \
        --i-reads rep-seqs.qza \
        --o-classification taxonomy.qza \
        --p-n-jobs 4 \
        --verbose && {
        TAXONOMY_SUCCESS=true
        log "‚úÖ Classification taxonomique r√©ussie"
    } || {
        log "‚ùå Classification √©chou√©e, cr√©ation taxonomie par d√©faut"
        create_dummy_taxonomy
    }
else
    log "‚ùå Classifieur non trouv√©"
    create_dummy_taxonomy
fi

# V√©rification finale
if [ "$TAXONOMY_SUCCESS" = true ] && [ -f "taxonomy.qza" ]; then
    log "‚úÖ Taxonomie disponible pour la suite du pipeline"
else
    log "‚ö† Taxonomie manquante - certaines analyses seront limit√©es"
    # Cr√©er un fichier vide pour √©viter erreurs ult√©rieures
    touch taxonomy.qza
fi

log "√âtape taxonomie termin√©e"

# ---- 08 ANALYSES FINALES ET EXPORTS
log "Analyses finales : core features, taxa barplots et exports"
mkdir -p "${ROOTDIR}/05_QIIME2/subtables" "${ROOTDIR}/05_QIIME2/export"

cd "${ROOTDIR}/05_QIIME2/core"

# Rar√©faction et analyse core features
log "Cr√©ation table rar√©fi√©e et analyse core features"

# ---- D√âTERMINATION PROFONDEUR RAR√âFACTION (SANS M√âTADONN√âES)
log "D√©termination profondeur de rar√©faction"

# Summary de la table SANS m√©tadonn√©es pour √©viter l'erreur
log "Summary sans m√©tadonn√©es pour √©viter conflits d'IDs"
conda run -n qiime2-2021.4 qiime feature-table summarize \
    --i-table table.qza \
    --o-visualization "../visual/table-summary.qzv" || {
    log "Erreur summary table"
    exit 1
}

# Export du summary pour extraction automatique
conda run -n qiime2-2021.4 qiime tools export \
    --input-path "../visual/table-summary.qzv" \
    --output-path "../visual/table-summary"

# Extraction automatique de la profondeur avec CONVERSION EN ENTIER
if [ -f "../visual/table-summary/sample-frequency-detail.csv" ]; then
    # Utiliser le 10√®me percentile et CONVERTIR EN ENTIER
    RAREFACTION_DEPTH_FLOAT=$(awk -F',' 'NR>1 {print $2}' "../visual/table-summary/sample-frequency-detail.csv" | sort -n | awk 'NR==int(NR*0.1)+1' || echo "5000")
    
    # CONVERSION CRUCIALE : float vers entier
    RAREFACTION_DEPTH=$(printf "%.0f" "$RAREFACTION_DEPTH_FLOAT" 2>/dev/null || echo "5000")
    
    # V√©rifier que c'est bien un entier positif
    if ! [[ "$RAREFACTION_DEPTH" =~ ^[0-9]+$ ]] || [ "$RAREFACTION_DEPTH" -lt 1 ]; then
        RAREFACTION_DEPTH=5000
        log "Valeur invalide d√©tect√©e, utilisation par d√©faut: $RAREFACTION_DEPTH"
    else
        log "Profondeur de rar√©faction automatique (entier): $RAREFACTION_DEPTH"
    fi
else
    RAREFACTION_DEPTH=5000
    log "Fichier summary non trouv√©, utilisation par d√©faut: $RAREFACTION_DEPTH"
fi

# Validation finale du type
log "Validation sampling-depth: '$RAREFACTION_DEPTH' (type: $(echo $RAREFACTION_DEPTH | awk '{print (int($1)==$1)?"entier":"float"}'))"

# Rar√©faction avec valeur enti√®re garantie
conda run -n qiime2-2021.4 qiime feature-table rarefy \
    --i-table table.qza \
    --p-sampling-depth "$RAREFACTION_DEPTH" \
    --o-rarefied-table "../subtables/RarTable-all.qza" || {
    log "Erreur rar√©faction, utilisez table originale"
    cp table.qza "../subtables/RarTable-all.qza"
}

log "‚úÖ Rar√©faction termin√©e avec depth=$RAREFACTION_DEPTH"

# Core features analysis
conda run -n qiime2-2021.4 qiime feature-table core-features \
    --i-table "../subtables/RarTable-all.qza" \
    --p-min-fraction 0.1 \
    --p-max-fraction 1.0 \
    --p-steps 10 \
    --o-visualization "../visual/CoreBiom-all.qzv" || {
    log "Erreur core features analysis"
}

# Taxa barplots
log "G√©n√©ration taxa barplots"

conda run -n qiime2-2021.4 qiime taxa barplot \
    --i-table table.qza \
    --i-taxonomy taxonomy.qza \
    --o-visualization "../visual/taxa-bar-plots.qzv" || {
    log "Erreur taxa barplots"
}

# ---- 08.02 M√âTRIQUES DE DIVERSIT√â COMPL√àTES
log "Calcul m√©triques de diversit√© alpha et beta avec PCoA et Emperor"
mkdir -p "${ROOTDIR}/05_QIIME2/diversity" "${ROOTDIR}/05_QIIME2/pcoa" 

cd "${ROOTDIR}/05_QIIME2/core"

# Cr√©ation arbre phylog√©n√©tique si n√©cessaire
log "G√©n√©ration arbre phylog√©n√©tique"
if [ ! -f "tree.qza" ]; then
    # Alignement multiple
    conda run -n qiime2-2021.4 qiime phylogeny align-to-tree-mafft-fasttree \
        --i-sequences rep-seqs.qza \
        --o-alignment aligned-rep-seqs.qza \
        --o-masked-alignment masked-aligned-rep-seqs.qza \
        --o-tree unrooted-tree.qza \
        --o-rooted-tree tree.qza \
        --p-n-threads "$NTHREADS" || {
        log "Erreur g√©n√©ration arbre phylog√©n√©tique"
    }
fi

# Cr√©ation m√©tadonn√©es minimales pour core-metrics
log "Cr√©ation m√©tadonn√©es automatiques pour diversit√©"
if [ ! -f "../98_databasefiles/diversity-metadata.tsv" ]; then
    # Export table pour obtenir les sample IDs
    conda run -n qiime2-2021.4 qiime tools export \
        --input-path table.qza \
        --output-path temp_diversity_export

    if [ -f "temp_diversity_export/feature-table.biom" ]; then
        # Extraire IDs avec Python ou alternative bash
        python3 -c "
try:
    import biom
    table = biom.load_table('temp_diversity_export/feature-table.biom')
    sample_ids = table.ids(axis='sample')
    
    with open('../98_databasefiles/diversity-metadata.tsv', 'w') as f:
        f.write('sample-id\\tgroup\\ttype\\n')
        for sid in sample_ids:
            if any(x in sid.lower() for x in ['neg', 'blank', 'control', 'eau']):
                f.write(f'{sid}\\tcontrol\\tnegative\\n')
            else:
                f.write(f'{sid}\\tsample\\tenvironmental\\n')
    print('M√©tadonn√©es diversit√© cr√©√©es')
except Exception as e:
    print(f'Erreur Python: {e}')
    exit(1)
" 2>/dev/null || {
        log "Cr√©ation m√©tadonn√©es bash alternative"
        biom summarize-table -i temp_diversity_export/feature-table.biom | \
        grep -A 1000 "Counts/sample detail" | \
        awk '/^[A-Za-z0-9]/ {print $1}' | head -50 > temp_sample_ids.txt
        
        echo -e "sample-id\tgroup\ttype" > "../98_databasefiles/diversity-metadata.tsv"
        while read -r sid; do
            if echo "${sid,,}" | grep -qE "(neg|blank|control|ctrl|eau)"; then
                echo -e "$sid\tcontrol\tnegative" >> "../98_databasefiles/diversity-metadata.tsv"
            else
                echo -e "$sid\tsample\tenvironmental" >> "../98_databasefiles/diversity-metadata.tsv"
            fi
        done < temp_sample_ids.txt
        rm temp_sample_ids.txt
    }
    rm -rf temp_diversity_export
    fi
fi

# Core metrics phylogenetic avec TOUS les outputs demand√©s
log "Lancement core-metrics-phylogenetic avec tous les outputs"
mkdir -p diversity pcoa visual

conda run -n qiime2-2021.4 qiime diversity core-metrics-phylogenetic \
    --i-table table.qza \
    --i-phylogeny tree.qza \
    --p-sampling-depth "$RAREFACTION_DEPTH" \
    --m-metadata-file "../98_databasefiles/diversity-metadata.tsv" \
    --output-dir diversity-results || {
    log "Erreur core-metrics-phylogenetic, utilisation core-metrics sans phylog√©nie"
    
    # Alternative sans phylog√©nie
    conda run -n qiime2-2021.4 qiime diversity core-metrics \
        --i-table table.qza \
        --p-sampling-depth "$RAREFACTION_DEPTH" \
        --m-metadata-file "../98_databasefiles/diversity-metadata.tsv" \
        --output-dir diversity-results || {
        log "Erreur core-metrics"
    }
}

# Copier et renommer les outputs selon vos sp√©cifications
if [ -d "diversity-results" ]; then
    log "Organisation des outputs de diversit√©"
    
    # Vecteurs alpha diversity
    [ -f "diversity-results/observed_features_vector.qza" ] && cp "diversity-results/observed_features_vector.qza" "diversity/Vector-observed_asv.qza"
    [ -f "diversity-results/shannon_vector.qza" ] && cp "diversity-results/shannon_vector.qza" "diversity/Vector-shannon.qza"
    [ -f "diversity-results/evenness_vector.qza" ] && cp "diversity-results/evenness_vector.qza" "diversity/Vector-evenness.qza"
    [ -f "diversity-results/faith_pd_vector.qza" ] && cp "diversity-results/faith_pd_vector.qza" "diversity/Vector-faith_pd.qza"
    
    # Matrices de distance
    [ -f "diversity-results/jaccard_distance_matrix.qza" ] && cp "diversity-results/jaccard_distance_matrix.qza" "diversity/Matrix-jaccard.qza"
    [ -f "diversity-results/bray_curtis_distance_matrix.qza" ] && cp "diversity-results/bray_curtis_distance_matrix.qza" "diversity/Matrix-braycurtis.qza"
    [ -f "diversity-results/unweighted_unifrac_distance_matrix.qza" ] && cp "diversity-results/unweighted_unifrac_distance_matrix.qza" "diversity/Matrix-unweighted_unifrac.qza"
    [ -f "diversity-results/weighted_unifrac_distance_matrix.qza" ] && cp "diversity-results/weighted_unifrac_distance_matrix.qza" "diversity/Matrix-weighted_unifrac.qza"
    
    # PCoA
    [ -f "diversity-results/jaccard_pcoa_results.qza" ] && cp "diversity-results/jaccard_pcoa_results.qza" "pcoa/PCoA-jaccard.qza"
    [ -f "diversity-results/bray_curtis_pcoa_results.qza" ] && cp "diversity-results/bray_curtis_pcoa_results.qza" "pcoa/PCoA-braycurtis.qza"
    [ -f "diversity-results/unweighted_unifrac_pcoa_results.qza" ] && cp "diversity-results/unweighted_unifrac_pcoa_results.qza" "pcoa/PCoA-unweighted_unifrac.qza"
    [ -f "diversity-results/weighted_unifrac_pcoa_results.qza" ] && cp "diversity-results/weighted_unifrac_pcoa_results.qza" "pcoa/PCoA-weighted_unifrac.qza"
    
    # Emperor plots
    [ -f "diversity-results/jaccard_emperor.qzv" ] && cp "diversity-results/jaccard_emperor.qzv" "visual/Emperor-jaccard.qzv"
    [ -f "diversity-results/bray_curtis_emperor.qzv" ] && cp "diversity-results/bray_curtis_emperor.qzv" "visual/Emperor-braycurtis.qzv"
    [ -f "diversity-results/unweighted_unifrac_emperor.qzv" ] && cp "diversity-results/unweighted_unifrac_emperor.qzv" "visual/Emperor-unweighted_unifrac.qzv"
    [ -f "diversity-results/weighted_unifrac_emperor.qzv" ] && cp "diversity-results/weighted_unifrac_emperor.qzv" "visual/Emperor-weighted_unifrac.qzv"
    
    log "‚úÖ Toutes les m√©triques de diversit√© g√©n√©r√©es et organis√©es"
fi


# ---- 09 EXPORTS QIIME2
log "Export de tous les fichiers QIIME2"
mkdir -p "${ROOTDIR}/05_QIIME2/export/core" \
         "${ROOTDIR}/05_QIIME2/export/subtables/RarTable-all" \
         "${ROOTDIR}/05_QIIME2/export/visual/CoreBiom-all" \
         "${ROOTDIR}/05_QIIME2/export/visual/taxa-bar-plots"

cd "${ROOTDIR}/05_QIIME2"

# Export table principale
log "Export table principale"
conda run -n qiime2-2021.4 qiime tools export \
    --input-path core/table.qza \
    --output-path export/core/table

# Export s√©quences repr√©sentatives
conda run -n qiime2-2021.4 qiime tools export \
    --input-path core/rep-seqs.qza \
    --output-path export/core/rep-seqs

# Export taxonomie
conda run -n qiime2-2021.4 qiime tools export \
    --input-path core/taxonomy.qza \
    --output-path export/core/taxonomy

# Export table rar√©fi√©e
conda run -n qiime2-2021.4 qiime tools export \
    --input-path subtables/RarTable-all.qza \
    --output-path export/subtables/RarTable-all

# Export visualisations
conda run -n qiime2-2021.4 qiime tools export \
    --input-path visual/CoreBiom-all.qzv \
    --output-path export/visual/CoreBiom-all || {
    log "Erreur export CoreBiom visualization"
}

conda run -n qiime2-2021.4 qiime tools export \
    --input-path visual/taxa-bar-plots.qzv \
    --output-path export/visual/taxa-bar-plots || {
    log "Erreur export taxa barplots"
}

# ---- 10 CONVERSIONS BIOM VERS TSV
log "Conversion BIOM vers TSV avec chemins corrects"
cd "${ROOTDIR}/05_QIIME2/export"

# S'assurer que les r√©pertoires existent
mkdir -p subtables/RarTable-all core/taxonomy

# Conversion table rar√©fi√©e
if [ -f "subtables/RarTable-all/feature-table.biom" ]; then
    log "Conversion table rar√©fi√©e BIOM vers TSV"
    biom convert \
        -i subtables/RarTable-all/feature-table.biom \
        -o subtables/RarTable-all/table-from-biom.tsv \
        --to-tsv || {
        log "Erreur conversion BIOM vers TSV"
    }
    
    # Modification header pour cr√©er ASV.tsv
    if [ -f "subtables/RarTable-all/table-from-biom.tsv" ]; then
        sed '1d ; s/#OTU ID/ASV_ID/' \
            subtables/RarTable-all/table-from-biom.tsv > \
            subtables/RarTable-all/ASV.tsv
        log "‚úÖ Fichier ASV.tsv cr√©√© : $(wc -l < subtables/RarTable-all/ASV.tsv) lignes"
    fi
else
    log "‚ùå Fichier BIOM manquant : subtables/RarTable-all/feature-table.biom"
fi

# Conversion table principale
if [ -f "core/table/feature-table.biom" ]; then
    log "Conversion table principale BIOM vers TSV"
    biom convert \
        -i core/table/feature-table.biom \
        -o core/table/table-from-biom.tsv \
        --to-tsv || {
        log "Erreur conversion BIOM principale vers TSV"
    }
    
    if [ -f "core/table/table-from-biom.tsv" ]; then
        sed '1d ; s/#OTU ID/ASV_ID/' \
            core/table/table-from-biom.tsv > \
            core/table/ASV.tsv
        log "‚úÖ Fichier ASV.tsv principal cr√©√©"
    fi
fi

# V√©rifier que taxonomy.tsv existe au bon endroit
if [ ! -f "core/taxonomy/taxonomy.tsv" ]; then
    log "‚ùå Fichier taxonomy.tsv manquant, tentative de localisation"
    find . -name "taxonomy.tsv" -type f 2>/dev/null | while read -r tax_file; do
        log "Trouv√© taxonomy.tsv √† : $tax_file"
        cp "$tax_file" core/taxonomy/taxonomy.tsv
        break
    done
fi


# ---- 11 CR√âATION FICHIER ASV AVEC TAXONOMIE (VERSION BASH COMPL√àTE)
log "Cr√©ation fichier ASV.txt avec taxonomie compl√®te (version bash)"
cd "${ROOTDIR}/05_QIIME2/export"

create_asv_with_taxonomy_bash() {
    local asv_file="subtables/RarTable-all/ASV.tsv"
    local taxonomy_file="core/taxonomy/taxonomy.tsv"
    local output_file="subtables/RarTable-all/ASV.txt"
    
    if [ ! -f "$asv_file" ] || [ ! -f "$taxonomy_file" ]; then
        log "‚ùå Fichiers requis manquants : $asv_file ou $taxonomy_file"
        return 1
    fi
    
    log "Traitement des fichiers ASV et taxonomie"
    
    # Obtenir header des √©chantillons depuis ASV.tsv
    sample_header=$(head -1 "$asv_file" | cut -f2-)
    
    # Cr√©er header final avec taxonomie
    echo -e "Kingdom\tPhylum\tClass\tOrder\tFamily\tGenus\tSpecies\t${sample_header}" > "$output_file"
    
    # Traiter chaque ASV
    tail -n +2 "$asv_file" | while IFS=$'\t' read -r asv_id asv_counts; do
        # Initialiser taxonomie par d√©faut
        kingdom="Unassigned"
        phylum="Unassigned"
        class="Unassigned"
        order="Unassigned"
        family="Unassigned"
        genus="Unassigned"
        species="Unassigned"
        
        # Chercher taxonomie dans fichier taxonomy.tsv
        if tax_line=$(grep "^${asv_id}" "$taxonomy_file" 2>/dev/null); then
            tax_string=$(echo "$tax_line" | cut -f2)
            
            # Parser la taxonomie QIIME2 (format : d__Bacteria; p__Proteobacteria; etc.)
            if [ -n "$tax_string" ]; then
                # S√©parer par ; et traiter chaque niveau
                IFS=';' read -ra tax_levels <<< "$tax_string"
                
                for level in "${tax_levels[@]}"; do
                    level=$(echo "$level" | xargs)  # Trim whitespace
                    
                    if [[ "$level" == d__* ]]; then
                        kingdom="${level#d__}"
                        kingdom="${kingdom:-Unassigned}"
                    elif [[ "$level" == p__* ]]; then
                        phylum="${level#p__}"
                        phylum="${phylum:-Unassigned}"
                    elif [[ "$level" == c__* ]]; then
                        class="${level#c__}"
                        class="${class:-Unassigned}"
                    elif [[ "$level" == o__* ]]; then
                        order="${level#o__}"
                        order="${order:-Unassigned}"
                    elif [[ "$level" == f__* ]]; then
                        family="${level#f__}"
                        family="${family:-Unassigned}"
                    elif [[ "$level" == g__* ]]; then
                        genus="${level#g__}"
                        genus="${genus:-Unassigned}"
                    elif [[ "$level" == s__* ]]; then
                        species="${level#s__}"
                        species="${species:-Unassigned}"
                    fi
                done
            fi
        fi
        
        # Nettoyer les valeurs vides
        [ -z "$kingdom" ] && kingdom="Unassigned"
        [ -z "$phylum" ] && phylum="Unassigned"
        [ -z "$class" ] && class="Unassigned"
        [ -z "$order" ] && order="Unassigned"
        [ -z "$family" ] && family="Unassigned"
        [ -z "$genus" ] && genus="Unassigned"
        [ -z "$species" ] && species="Unassigned"
        
        # √âcrire ligne finale
        echo -e "${kingdom}\t${phylum}\t${class}\t${order}\t${family}\t${genus}\t${species}\t${asv_counts}" >> "$output_file"
    done
    
    log "‚úÖ Fichier ASV.txt cr√©√© avec taxonomie compl√®te"
    log "Lignes dans le fichier final: $(wc -l < "$output_file")"
    
    # Afficher un √©chantillon du r√©sultat
    log "Aper√ßu du fichier ASV.txt:"
    head -3 "$output_file" | column -t -s$'\t' 2>/dev/null || head -3 "$output_file"
}

# Ex√©cuter la fonction
create_asv_with_taxonomy_bash || {
    log "‚ùå Cr√©ation ASV.txt bash √©chou√©e"
    
    # Version simplifi√©e finale de secours
    if [ -f "subtables/RarTable-all/ASV.tsv" ]; then
        log "Version de secours ultra-simplifi√©e"
        sample_header=$(head -1 "subtables/RarTable-all/ASV.tsv" | cut -f2-)
        echo -e "Kingdom\tPhylum\tClass\tOrder\tFamily\tGenus\tSpecies\t${sample_header}" > "subtables/RarTable-all/ASV.txt"
        
        tail -n +2 "subtables/RarTable-all/ASV.tsv" | while IFS=$'\t' read -r asv_id asv_counts; do
            echo -e "Bacteria\tUnknown\tUnknown\tUnknown\tUnknown\tUnknown\tUnknown\t${asv_counts}" >> "subtables/RarTable-all/ASV.txt"
        done
        
        log "‚úÖ Fichier ASV.txt cr√©√© (version simplifi√©e)"
    fi
}

# ---- 12 CR√âATION TABLEAUX R√âCAPITULATIFS DE M√âTRIQUES
log "Cr√©ation tableaux r√©capitulatifs de toutes les m√©triques"
mkdir -p "${ROOTDIR}/05_QIIME2/export/summary_tables"
cd "${ROOTDIR}/05_QIIME2/export"

# Fonction pour extraire m√©triques FastQC
extract_fastqc_metrics() {
    local fastqc_dir="$1"
    local output_prefix="$2"
    local output_file="summary_tables/${output_prefix}_fastqc_metrics.tsv"
    
    log "Extraction m√©triques FastQC depuis $fastqc_dir"
    
    # Header du tableau
    echo -e "Sample\tTotal_Sequences\tSequences_Flagged_Poor_Quality\tSequence_Length\tGC_Content\tTotal_Duplicate_Percentage" > "$output_file"
    
    # Parser chaque fichier FastQC
    find "$fastqc_dir" -name "*_fastqc.zip" 2>/dev/null | while read -r zip_file; do
        if [ -f "$zip_file" ]; then
            sample=$(basename "$zip_file" _fastqc.zip)
            
            # Extraire donn√©es du zip
            temp_dir=$(mktemp -d)
            unzip -q "$zip_file" -d "$temp_dir" 2>/dev/null || continue
            
            # Chercher le fichier fastqc_data.txt
            data_file=$(find "$temp_dir" -name "fastqc_data.txt" 2>/dev/null | head -1)
            
            if [ -f "$data_file" ]; then
                # Extraire m√©triques cl√©s
                total_seq=$(grep "Total Sequences" "$data_file" | awk -F'\t' '{print $2}' || echo "NA")
                poor_qual=$(grep "Sequences flagged as poor quality" "$data_file" | awk -F'\t' '{print $2}' || echo "0")
                seq_len=$(grep "Sequence length" "$data_file" | awk -F'\t' '{print $2}' || echo "NA")
                gc_content=$(grep "%GC" "$data_file" | awk -F'\t' '{print $2}' || echo "NA")
                duplicates=$(grep "Total Duplicate Percentage" "$data_file" | awk -F'\t' '{print $2}' || echo "NA")
                
                echo -e "$sample\t$total_seq\t$poor_qual\t$seq_len\t$gc_content\t$duplicates" >> "$output_file"
            fi
            
            rm -rf "$temp_dir"
        fi
    done
    
    log "‚úÖ M√©triques FastQC extraites : $output_file ($(wc -l < "$output_file" 2>/dev/null || echo "0") lignes)"
}

# Extraire m√©triques FastQC avant nettoyage
if [ -d "${ROOTDIR}/02_qualitycheck" ]; then
    extract_fastqc_metrics "${ROOTDIR}/02_qualitycheck" "raw_data"
fi

# Extraire m√©triques FastQC apr√®s nettoyage
if [ -d "${ROOTDIR}/03_cleaned_data_qc" ]; then
    extract_fastqc_metrics "${ROOTDIR}/03_cleaned_data_qc" "cleaned_data"
fi

# Cr√©er tableau r√©capitulatif DADA2 et QIIME2
log "Cr√©ation tableau r√©capitulatif DADA2 et m√©triques QIIME2"

create_qiime_metrics_summary() {
    local output_file="summary_tables/qiime2_pipeline_metrics.tsv"
    
    echo -e "Metric_Type\tMetric_Name\tValue\tFile_Source" > "$output_file"
    
    # M√©triques DADA2 si disponibles
    if [ -f "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" ] || \
       [ -f "${ROOTDIR}/05_QIIME2/core/denoising-stats.qza" ]; then
        
        # Export DADA2 stats si pas d√©j√† fait
        if [ -f "${ROOTDIR}/05_QIIME2/core/denoising-stats.qza" ] && [ ! -f "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" ]; then
            mkdir -p "${ROOTDIR}/05_QIIME2/export/core/denoising-stats"
            conda run -n qiime2-2021.4 qiime tools export \
                --input-path "${ROOTDIR}/05_QIIME2/core/denoising-stats.qza" \
                --output-path "${ROOTDIR}/05_QIIME2/export/core/denoising-stats" 2>/dev/null || true
        fi
        
        if [ -f "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" ]; then
            # Parser stats DADA2
            total_samples=$(tail -n +2 "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" | wc -l || echo "0")
            
            # Moyennes des colonnes num√©riques
            if [ "$total_samples" -gt 0 ]; then
                avg_input=$(tail -n +2 "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" | awk -F'\t' '{sum+=$2; count++} END {if(count>0) print sum/count; else print "NA"}')
                avg_filtered=$(tail -n +2 "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" | awk -F'\t' '{sum+=$3; count++} END {if(count>0) print sum/count; else print "NA"}')
                avg_denoised=$(tail -n +2 "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" | awk -F'\t' '{sum+=$5; count++} END {if(count>0) print sum/count; else print "NA"}')
                avg_merged=$(tail -n +2 "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" | awk -F'\t' '{sum+=$6; count++} END {if(count>0) print sum/count; else print "NA"}')
                avg_nonchimeric=$(tail -n +2 "${ROOTDIR}/05_QIIME2/export/core/denoising-stats/stats.tsv" | awk -F'\t' '{sum+=$7; count++} END {if(count>0) print sum/count; else print "NA"}')
                
                echo -e "DADA2\tTotal_Samples\t$total_samples\tdenoising-stats.tsv" >> "$output_file"
                echo -e "DADA2\tAvg_Input_Reads\t$avg_input\tdenoising-stats.tsv" >> "$output_file"
                echo -e "DADA2\tAvg_Filtered_Reads\t$avg_filtered\tdenoising-stats.tsv" >> "$output_file"
                echo -e "DADA2\tAvg_Denoised_Reads\t$avg_denoised\tdenoising-stats.tsv" >> "$output_file"
                echo -e "DADA2\tAvg_Merged_Reads\t$avg_merged\tdenoising-stats.tsv" >> "$output_file"
                echo -e "DADA2\tAvg_NonChimeric_Reads\t$avg_nonchimeric\tdenoising-stats.tsv" >> "$output_file"
            fi
        fi
    fi
    
    # M√©triques de la table de features
    if [ -f "core/table/feature-table.biom" ]; then
        total_features=$(biom summarize-table -i core/table/feature-table.biom | grep "Num observations:" | awk '{print $3}' || echo "NA")
        total_samples_table=$(biom summarize-table -i core/table/feature-table.biom | grep "Num samples:" | awk '{print $3}' || echo "NA")
        total_counts=$(biom summarize-table -i core/table/feature-table.biom | grep "Total count:" | awk '{print $3}' || echo "NA")
        
        echo -e "FeatureTable\tTotal_Features_ASVs\t$total_features\tfeature-table.biom" >> "$output_file"
        echo -e "FeatureTable\tTotal_Samples\t$total_samples_table\tfeature-table.biom" >> "$output_file"
        echo -e "FeatureTable\tTotal_Sequence_Count\t$total_counts\tfeature-table.biom" >> "$output_file"
    fi
    
    # M√©triques taxonomie
    if [ -f "core/taxonomy/taxonomy.tsv" ]; then
        total_classified=$(tail -n +2 core/taxonomy/taxonomy.tsv | wc -l || echo "0")
        classified_bacteria=$(grep -i "bacteria" core/taxonomy/taxonomy.tsv | wc -l || echo "0")
        classified_archaea=$(grep -i "archaea" core/taxonomy/taxonomy.tsv | wc -l || echo "0")
        unclassified=$(grep -i "unassigned\|unknown" core/taxonomy/taxonomy.tsv | wc -l || echo "0")
        
        echo -e "Taxonomy\tTotal_Classified_ASVs\t$total_classified\ttaxonomy.tsv" >> "$output_file"
        echo -e "Taxonomy\tBacteria_ASVs\t$classified_bacteria\ttaxonomy.tsv" >> "$output_file"
        echo -e "Taxonomy\tArchaea_ASVs\t$classified_archaea\ttaxonomy.tsv" >> "$output_file"
        echo -e "Taxonomy\tUnclassified_ASVs\t$unclassified\ttaxonomy.tsv" >> "$output_file"
    fi
    
    log "‚úÖ Tableau m√©triques QIIME2 cr√©√© : $output_file"
}

create_qiime_metrics_summary

# Cr√©er tableau r√©capitulatif des m√©triques de diversit√©
create_diversity_metrics_summary() {
    local output_file="summary_tables/diversity_metrics_summary.tsv"
    local diversity_dir="${ROOTDIR}/05_QIIME2"
    
    echo -e "Diversity_Type\tMetric_Name\tFile_Type\tFile_Path\tStatus" > "$output_file"
    
    # V√©rifier pr√©sence des fichiers de diversit√©
    diversity_files=(
        "Alpha:Observed_Features:qza:diversity/Vector-observed_asv.qza"
        "Alpha:Shannon:qza:diversity/Vector-shannon.qza"
        "Alpha:Evenness:qza:diversity/Vector-evenness.qza"
        "Alpha:Faith_PD:qza:diversity/Vector-faith_pd.qza"
        "Beta:Jaccard_Distance:qza:diversity/Matrix-jaccard.qza"
        "Beta:BrayCurtis_Distance:qza:diversity/Matrix-braycurtis.qza"
        "Beta:UnweightedUniFrac:qza:diversity/Matrix-unweighted_unifrac.qza"
        "Beta:WeightedUniFrac:qza:diversity/Matrix-weighted_unifrac.qza"
        "PCoA:Jaccard_PCoA:qza:pcoa/PCoA-jaccard.qza"
        "PCoA:BrayCurtis_PCoA:qza:pcoa/PCoA-braycurtis.qza"
        "PCoA:UnweightedUniFrac_PCoA:qza:pcoa/PCoA-unweighted_unifrac.qza"
        "PCoA:WeightedUniFrac_PCoA:qza:pcoa/PCoA-weighted_unifrac.qza"
        "Visualization:Jaccard_Emperor:qzv:visual/Emperor-jaccard.qzv"
        "Visualization:BrayCurtis_Emperor:qzv:visual/Emperor-braycurtis.qzv"
        "Visualization:UnweightedUniFrac_Emperor:qzv:visual/Emperor-unweighted_unifrac.qzv"
        "Visualization:WeightedUniFrac_Emperor:qzv:visual/Emperor-weighted_unifrac.qzv"
    )
    
    for entry in "${diversity_files[@]}"; do
        IFS=':' read -r div_type metric_name file_type file_path <<< "$entry"
        full_path="${diversity_dir}/${file_path}"
        
        if [ -f "$full_path" ]; then
            status="Present"
            # Taille du fichier pour les .qza
            if [[ "$file_type" == "qza" ]]; then
                size=$(ls -lh "$full_path" | awk '{print $5}')
                status="Present ($size)"
            fi
        else
            status="Missing"
        fi
        
        echo -e "$div_type\t$metric_name\t$file_type\t$file_path\t$status" >> "$output_file"
    done
    
    log "‚úÖ Tableau m√©triques diversit√© cr√©√© : $output_file"
}

create_diversity_metrics_summary

# Cr√©er un rapport de synth√®se final
log "Cr√©ation rapport de synth√®se final"
cat > "summary_tables/PIPELINE_SUMMARY_REPORT.md" << 'EOF'
# Rapport de Synth√®se Pipeline QIIME2 Valormicro

## Fichiers g√©n√©r√©s

### Tables principales
- **ASV Table avec taxonomie** : `subtables/RarTable-all/ASV.txt`
- **Table de features BIOM** : `core/table/feature-table.biom`
- **Taxonomie** : `core/taxonomy/taxonomy.tsv`
- **S√©quences repr√©sentatives** : `core/rep-seqs/dna-sequences.fasta`

### M√©triques de diversit√©
- **Alpha diversity** : Vector-observed_asv.qza, Vector-shannon.qza, Vector-evenness.qza, Vector-faith_pd.qza
- **Beta diversity** : Matrix-jaccard.qza, Matrix-braycurtis.qza, Matrix-unweighted_unifrac.qza, Matrix-weighted_unifrac.qza
- **PCoA** : PCoA-jaccard.qza, PCoA-braycurtis.qza, PCoA-unweighted_unifrac.qza, PCoA-weighted_unifrac.qza
- **Visualisations Emperor** : Emperor-jaccard.qzv, Emperor-braycurtis.qzv, Emperor-unweighted_unifrac.qzv, Emperor-weighted_unifrac.qzv

### Rapports qualit√©
- **FastQC donn√©es brutes** : `../../02_qualitycheck/raw_data_qc.html`
- **FastQC donn√©es nettoy√©es** : `../../03_cleaned_data_qc/cleaned_data_qc.html`
- **Taxa barplots** : `visual/taxa-bar-plots.qzv`
- **Core features** : `visual/CoreBiom-all.qzv`

### Tableaux r√©capitulatifs
- **M√©triques FastQC brutes** : `summary_tables/raw_data_fastqc_metrics.tsv`
- **M√©triques FastQC nettoy√©es** : `summary_tables/cleaned_data_fastqc_metrics.tsv`
- **M√©triques pipeline QIIME2** : `summary_tables/qiime2_pipeline_metrics.tsv`
- **M√©triques diversit√©** : `summary_tables/diversity_metrics_summary.tsv`

## Utilisation des fichiers

### Pour analyses statistiques
Utilisez `ASV.txt` qui contient les comptages avec taxonomie compl√®te.

### Pour visualisations
Les fichiers `.qzv` peuvent √™tre visualis√©s sur https://view.qiime2.org

### Pour analyses phylog√©n√©tiques
Utilisez `tree.qza` avec les m√©triques UniFrac.
EOF

log "üéâ TOUS LES TABLEAUX R√âCAPITULATIFS CR√â√âS !"
log "Consultez le r√©pertoire export/summary_tables/ pour tous les r√©sum√©s"
log "Rapport principal : export/summary_tables/PIPELINE_SUMMARY_REPORT.md"


log "üèÅ PIPELINE COMPLET TERMIN√â AVEC SUCC√àS !"
log "Fichiers g√©n√©r√©s:"
log "- Table DADA2: ${ROOTDIR}/05_QIIME2/core/table.qza"
log "- Taxonomie: ${ROOTDIR}/05_QIIME2/core/taxonomy.qza"
log "- Core features: ${ROOTDIR}/05_QIIME2/visual/CoreBiom-all.qzv"
log "- Taxa barplots: ${ROOTDIR}/05_QIIME2/visual/taxa-bar-plots.qzv"
log "- ASV avec taxonomie: ${ROOTDIR}/05_QIIME2/export/subtables/RarTable-all/ASV.txt"
log "- Tous les exports dans: ${ROOTDIR}/05_QIIME2/export/"
